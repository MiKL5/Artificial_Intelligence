# **Les benchmarks complétant GLUE**
Leur but est de fournir une évaluation plus complète des capacités des modèles de traitement du langage naturel (PLN). 
## **Les principaux benchmarks**
1. **SuperGLUE** est un ensemble de tâches plus difficiles et plus diversifiées que GLUE, pour évaluer des capacités de compréhension plus avancées du langage.  
Il inclut des tâches comme la résolution d’ambiguïtés, le raisonnement commun et le dialogue.
2. **DELPHI (Diagnostic Evaluation of Language PragmatIcs)** évalue la capacité des modèles à comprendre et à utiliser le langage de manière pragmatique, en tenant compte du contexte.  
Incluantt des tâches liées à l’interprétation des actes de langage, des implicatures et des expressions idiomatiques.  
3. CLUE (Chinese Language Understanding Evaluation) est un benchmark similaire à GLUE, mais spécifiquement conçu pour évaluer les modèles PLN en langue chinoise.  
Et inclut des tâches telles que la classification de sentiment, l’inférence textuelle et la détection de paraphrase.
4. **GLUE-Diagnosis** est un complément à GLUE, pour mieux comprendre les frces et les faiblesses des modèles PLN.  
Il implique des tâches de diagnostic plus spécifiques, comme l’identification des erreurs de raisonnement ou de connaissances factuelles.
5. **GROCSE (General Reading Comprehension Snapshot)** évalue la capacité des modèles à comprendre et à raisonner sur des textes longs et complexes, en utilisant des tâches de compréhension de lecture.  
Inclut des textes provenant de sources diverses, comme des articles de presse et des livres.

_**⟹ Ces benchmarks, en complément de GLUE, permettent d’évaluer les modèles PLN de manière plus approfondie et de guider les efforts de recherche vers des capacités de compréhension du langage toujours plus sophistiquées.**_