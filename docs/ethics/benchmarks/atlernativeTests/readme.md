# **Quelques alternatives**
Ces approches complémentaires permettent d’évaluer de manière plus approfondie les facettes de la compréhension dans les systèmes d’IA.
1. **Les tests de raisonnement et de résolution de problèmes**
    * Le benchmark GLUE (General Language Understanding Evaluation) évalue la capacité des modèles de langage à résoudre une variété de tâches de compréhension du langage naturel. Pour cela il faut concevoir des tâches qui nécessitent une compréhension approfondie du contexte et du sens, comme la résolution d’analogies complexes, comme la classification de textes, la compréhension de textes et la prise de décisions dans des scénarios complexes.  
    * Évaluer la capacité du système à raisonner de manière logique et à généraliser à de nouvelles situations.
    * Le _jeu de questions VQA (Visual Question Answering)_ teste la capacité des modèles d’IA à répondre à des questions sur le contenu d’images.
2. **Tests d’explication et de justification**
    * Demander au système d’expliquer ses raisonnements et ses décisions de manière détaillée et compréhensible.
        * Le framework XAI (Explainable AI) développe des techniques permettant aux systèmes d’IA d’expliquer leurs décisions de manière compréhensible aux humains.
    * Des études demandent à des agents conversationnels d’expliquer leurs raisonnements de manière détaillée afin d’évaluer si le système est capable de justifier ses actions de façon cohérente et convaincante.
3. **Tests d’apprentissage actif et de curiosité**
    * Concevoir des environnements d’apprentissage par renforcement, comme les jeux vidéo, évaluent la capacité des agents à poser des questions pertinentes pour approfondir leur compréhension.
    * Des tâches d’exploration active mesurent la capacité des agents à découvrir de nouvelles informations de manière autonome.
4. **Tests de transfert de connaissances**
    * Les benchmarks de “zero-shot learning” évaluent la capacité des modèles à résoudre des tâches sur des données jamais vues pendant l’entraînement.
    * Des études testent la capacité des agents à appliquer leurs connaissances à de nouveaux domaines et situations.
5. **Tests d’interaction sociale et d’empathie**
    * Des scénarios d’interaction conversationnelle évaluent la capacité des agents à adapter leur réponse au contexte social et aux émotions exprimées par les utilisateurs.
    * Des tâches d’assistance personnelle mesurent la capacité des agents à comprendre et à répondre de manière appropriée aux besoins spécifiques des utilisateurs dans des situations sociales complexes.